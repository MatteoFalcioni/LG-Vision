{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f549f80",
   "metadata": {},
   "source": [
    "# Parallel Processing for Street View Analysis\n",
    "\n",
    "## Overview\n",
    "This notebook implements parallel processing for analyzing thousands of Street View images using LangGraph with multiple concurrent nodes.\n",
    "\n",
    "## Problem Statement\n",
    "- **Scale**: Thousands of locations × multiple orientations (horizon/ground/sky) = 10k+ images\n",
    "- **Bottleneck**: Sequential AI model calls would take hours\n",
    "- **Solution**: Parallel processing with bounded concurrency\n",
    "\n",
    "## Architecture\n",
    "- **Input**: CSV with Street View URLs (from `google_apis.ipynb`)\n",
    "- **Processing**: LangGraph parallel nodes for vision analysis\n",
    "- **Output**: Graded/analyzed results per image\n",
    "\n",
    "## Concurrency Strategy\n",
    "- **Measure**: Single call latency (L) for vision model\n",
    "- **Calculate**: k ≈ T × L (where T = target throughput)\n",
    "- **Start**: k=16 nodes, scale up based on provider limits\n",
    "- **Monitor**: 429 errors, latency percentiles, success rates\n",
    "\n",
    "## Key Considerations\n",
    "- **Provider Limits**: API rate limits and concurrent request caps\n",
    "- **Rate Limiting**: Client-side throttling to avoid 429s\n",
    "- **Retry Logic**: Exponential backoff with jitter\n",
    "- **Checkpointing**: Idempotent tasks to avoid duplicate billing\n",
    "- **Budget Guards**: Max requests per minute/hour\n",
    "\n",
    "## Implementation Plan\n",
    "1. Load CSV and create work queue\n",
    "2. Implement bounded concurrency with LangGraph\n",
    "3. Add retry logic and rate limiting\n",
    "4. Monitor metrics and scale accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc118e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path().absolute().parent))  # This makes the parent directory available so you can use clean absolute imports like from src.graph import ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b5f181",
   "metadata": {},
   "source": [
    "Idea: change the state to be a dict of key = `point_id` and value = dict containing `{horizontal: [urls] , ground : [urls], sky : [url]}`   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e050d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentState\n",
    "from typing import Union\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "def add_dict(left : Union[dict[str, dict[str, list[str]]], None], right : Union[dict[str, dict[str, list[str]]], None]) -> dict[str, dict[str, list[str]]]:\n",
    "    \"\"\"\n",
    "    Reducer to combine two dictionaries. Used for streetview urls and grades.\n",
    "\n",
    "    The streetview urls dictionary is of the form:\n",
    "    {\n",
    "        \"point_id\" : {\n",
    "            \"horizon\" : [urls],\n",
    "            \"ground\" : [urls],\n",
    "            \"sky\" : [url]  # (unique url)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    The grades dictionary is of the form:\n",
    "    {\n",
    "        \"point_id\" : {\n",
    "            \"horizon\" : int,\n",
    "            \"ground\" : int,\n",
    "            \"sky\" : int\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    if left is None:  # init left list\n",
    "        left = {}\n",
    "    \n",
    "    if right is None:   # init right list\n",
    "        right = {}\n",
    "    \n",
    "    return {**left, **right}  # merge the two dictionaries\n",
    "\n",
    "\n",
    "class StreetViewState(AgentState):\n",
    "    \"\"\"State of multimodal agent, inherits from AgentState, so it gets `messages` and `remaining_steps` keys for free\"\"\"\n",
    "    streetviews : Annotated[dict[str, dict[str, list[str]]], add_dict]\n",
    "    results : Annotated[dict[str, dict[str, Union[int, str]]], add_dict] # will be like {point_id : {grade : int, description : str}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e5b5ab",
   "metadata": {},
   "source": [
    "Then, each agent processes a single point's views. Then we take the mean of the grades the agent gives out - by the way, we need structured outputs.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac62450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import base64\n",
    "import requests\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "\n",
    "def prepare_input(state: StreetViewState) -> List[HumanMessage]:\n",
    "    \"\"\"\n",
    "    Build one HumanMessage per **horizon image** (prototype) in the pre-assigned state.\n",
    "    Each message contains: prompt text + exactly one image.\n",
    "    \n",
    "    Returns:\n",
    "        List[HumanMessage]\n",
    "    \"\"\"\n",
    "    messages: List[HumanMessage] = []\n",
    "\n",
    "    text = (\n",
    "        \"Analyze this Street View horizon image and grade the scene from 1 to 10. \"\n",
    "        \"Consider urban quality, cleanliness, architectural appeal, and overall visual pleasantness.\"\n",
    "    )\n",
    "\n",
    "    for point_id, views in state.streetviews.items():\n",
    "        for url in views.get(\"horizon\", []):\n",
    "            try:\n",
    "                resp = requests.get(url, timeout=10)\n",
    "                resp.raise_for_status()\n",
    "                img_b64 = base64.b64encode(resp.content).decode(\"utf-8\")\n",
    "                mime_type = resp.headers.get(\"content-type\", \"image/jpeg\")\n",
    "\n",
    "                content_blocks = [\n",
    "                    {\"type\": \"text\", \"text\": text},\n",
    "                    {\"type\": \"image\", \"base64\": img_b64, \"mime_type\": mime_type},\n",
    "                ]\n",
    "                messages.append(HumanMessage(content=content_blocks))\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Failed to download image from {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ffa53",
   "metadata": {},
   "source": [
    "We also want the agent to output a structured output: just the grade from 1 to 10 that he gave to the scene, maybe with `point_id` as well - but we can add the point id later manually, parsing it from state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819416d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GradeOutput(BaseModel):\n",
    "    \"\"\"\n",
    "    Grade given to the scene\n",
    "    \"\"\"\n",
    "    grade : int = Field(description=\"The grade from 1 to 10 that the agent gave to the scene\")\n",
    "    description : str = Field(description=\"A description of the scene\")\n",
    "\n",
    "# then extract with result= agent.invoke(...), result[\"structured_response\"] will be a GradeOutput object: GradeOutput(grade=5, description=\"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a2cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OPENAI model\n"
     ]
    }
   ],
   "source": [
    "from src.prompts import multimodal_prompt\n",
    "from src.models import get_multimodal_model\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain.agents import create_agent\n",
    "import pandas as pd\n",
    "\n",
    "multimodal_model = get_multimodal_model()\n",
    "multimodal_agent = create_agent(\n",
    "    model=multimodal_model,\n",
    "    tools=[],\n",
    "    system_prompt=multimodal_prompt,\n",
    "    response_format=GradeOutput\n",
    ")\n",
    "\n",
    "async def multimodal_node(state: StreetViewState):   \n",
    "    \"\"\"\n",
    "    Invokes the agent on the input messages. \n",
    "    For horizon images, we want to invoke one by one and then take the mean of the grades manually.\n",
    "    \"\"\"\n",
    "    # construct multimodal input message\n",
    "    multimodal_msg = prepare_input(state)  # returns HumanMessage with image\n",
    "\n",
    "    result = await multimodal_agent.ainvoke({\"messages\": multimodal_msg})\n",
    "    grade = result[\"structured_response\"].grade\n",
    "    description = result[\"structured_response\"].description\n",
    "\n",
    "    return {\"results\" : {state.point_id : {\"grade\" : grade, \"description\" : description}}},  \n",
    "\n",
    "def save_results(state: StreetViewState):\n",
    "    \"\"\"\n",
    "    Save the results to a new csv file\n",
    "    \"\"\"\n",
    "    csv_path = Path(\"./streetview_samples.csv\")\n",
    "    \n",
    "    # read the original csv\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # add the results to the df\n",
    "    df = df.merge(state.results, left_index=True, right_index=True)\n",
    "    # save the df to a new csv file\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    return f\"Results saved to {csv_path}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(checkpointer, save_display=False) -> StateGraph:\n",
    "    \"\"\"\n",
    "    Get the builder for the graph\n",
    "    \"\"\"\n",
    "    builder = StateGraph(StreetViewState)\n",
    "    # nodes\n",
    "    builder.add_node(\"multimodal_agent\", multimodal_node)\n",
    "    builder.add_node(\"save_results\", save_results)\n",
    "    builder.add_node(\"prepare_input\", prepare_input)\n",
    "    # edges\n",
    "    builder.add_edge(START, \"prepare_input\")\n",
    "    builder.add_edge(\"prepare_input\", \"multimodal_agent\")\n",
    "    builder.add_edge(\"multimodal_agent\", \"save_results\")\n",
    "\n",
    "    graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "    if save_display:\n",
    "        # save the graph display to file\n",
    "        img = graph.get_graph().draw_mermaid_png() # returns bytes\n",
    "        # save the bytes to file \n",
    "        with open(\"./graph.png\", \"wb\") as f:\n",
    "            f.write(img)\n",
    "        print(\"Graph display saved to ./src/graph.png\")\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9836db",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da2c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path(\"./streetview_samples.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
